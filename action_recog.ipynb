{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Garg_Utkarsh_112672834_hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AcamCuuyjYb2"
      },
      "source": [
        "# Action Recognition @ UCF101  \n",
        "**Due date: 11:59 pm on Nov. 19, 2019 (Tuesday)**\n",
        "\n",
        "## Description\n",
        "---\n",
        "In this homework, you will be doing action recognition using Recurrent Neural Network (RNN), (Long-Short Term Memory) LSTM in particular. You will be given a dataset called UCF101, which consists of 101 different actions/classes and for each action, there will be 145 samples. We tagged each sample into either training or testing. Each sample is supposed to be a short video, but we sampled 25 frames from each videos to reduce the amount of data. Consequently, a training sample is an image tuple that forms a 3D volume with one dimension encoding *temporal correlation* between frames and a label indicating what action it is.\n",
        "\n",
        "To tackle this problem, we aim to build a neural network that can not only capture spatial information of each frame but also temporal information between frames. Fortunately, you don't have to do this on your own. RNN — a type of neural network designed to deal with time-series data — is right here for you to use. In particular, you will be using LSTM for this task.\n",
        "\n",
        "Instead of training an end-to-end neural network from scratch whose computation is prohibitively expensive, we divide this into two steps: feature extraction and modelling. Below are the things you need to implement for this homework:\n",
        "- **{35 pts} Feature extraction**. Use any of the [pre-trained models](https://pytorch.org/docs/stable/torchvision/models.html) to extract features from each frame. Specifically, we recommend not to use the activations of the last layer as the features tend to be task specific towards the end of the network. \n",
        "    **hints**: \n",
        "    - A good starting point would be to use a pre-trained VGG16 network, we suggest first fully connected layer `torchvision.models.vgg16` (4096 dim) as features of each video frame. This will result into a 4096x25 matrix for each video. \n",
        "    - Normalize your images using `torchvision.transforms` \n",
        "    ```\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
        "    prep(img)\n",
        "    The mean and std. mentioned above is specific to Imagenet data\n",
        "    \n",
        "    ```\n",
        "    More details of image preprocessing in PyTorch can be found at http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "    \n",
        "- **{35 pts} Modelling**. With the extracted features, build an LSTM network which takes a **dx25** sample as input (where **d** is the dimension of the extracted feature for each frame), and outputs the action label of that sample.\n",
        "- **{20 pts} Evaluation**. After training your network, you need to evaluate your model with the testing data by computing the prediction accuracy **(5 points)**. The baseline test accuracy for this data is 75%, and **10 points** out of 20 is for achieving test accuracy greater than the baseline. Moreover, you need to compare **(5 points)** the result of your network with that of support vector machine (SVM) (stacking the **dx25** feature matrix to a long vector and train a SVM).\n",
        "- **{10 pts} Report**. Details regarding the report can be found in the submission section below.\n",
        "\n",
        "Notice that the size of the raw images is 256x340, whereas your pre-trained model might take **nxn** images as inputs. To solve this problem, instead of resizing the images which unfavorably changes the spatial ratio, we take a better solution: Cropping five **nxn** images, one at the image center and four at the corners and compute the **d**-dim features for each of them, and average these five **d**-dim feature to get a final feature representation for the raw image.\n",
        "For example, VGG takes 224x224 images as inputs, so we take the five 224x224 croppings of the image, compute 4096-dim VGG features for each of them, and then take the mean of these five 4096-dim vectors to be the representation of the image.\n",
        "\n",
        "In order to save you computational time, you need to do the classification task only for **the first 25** classes of the whole dataset. The same applies to those who have access to GPUs. **Bonus 10 points for running and reporting on the entire 101 classes.**\n",
        "\n",
        "\n",
        "## Dataset\n",
        "Download **dataset** at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar)(Image data for each video) and the **annos folder** which has the video labels and the label to class name mapping is included in the assignment folder uploaded. \n",
        "\n",
        "\n",
        "UCF101 dataset contains 101 actions and 13,320 videos in total.  \n",
        "\n",
        "+ `annos/actions.txt`  \n",
        "  + lists all the actions (`ApplyEyeMakeup`, .., `YoYo`)   \n",
        "  \n",
        "+ `annots/videos_labels_subsets.txt`  \n",
        "  + lists all the videos (`v_000001`, .., `v_013320`)  \n",
        "  + labels (`1`, .., `101`)  \n",
        "  + subsets (`1` for train, `2` for test)  \n",
        "\n",
        "+ `images/`  \n",
        "  + each folder represents a video\n",
        "  + the video/folder name to class mapping can be found using `annots/videos_labels_subsets.txt`, for e.g. `v_000001` belongs to class 1 i.e. `ApplyEyeMakeup`\n",
        "  + each video folder contains 25 frames  \n",
        "\n",
        "\n",
        "\n",
        "## Some Tutorials\n",
        "- Good materials for understanding RNN and LSTM\n",
        "    - http://blog.echen.me\n",
        "    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- Implementing RNN and LSTM with PyTorch\n",
        "    - [LSTM with PyTorch](http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)\n",
        "    - [RNN with PyTorch](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvHKxzKJuy8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import pandas as pd\n",
        "import random \n",
        "import time\n",
        "import pickle\n",
        "import torch\n",
        "import torch.utils.data as DD\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import warnings\n",
        "# import cPickle\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import os\n",
        "from scipy import io\n",
        "from scipy.io import savemat, loadmat\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UdCRwkL7jxtc"
      },
      "source": [
        "---\n",
        "---\n",
        "## **Problem 1.** Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvW6USdEuz6j",
        "colab_type": "code",
        "outputId": "dd93f4c8-403e-43de-9cbe-1ffdd9739b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount your google drive where you've saved your assignment folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yu0E65FRTXM",
        "colab_type": "code",
        "outputId": "456d01b8-eabe-40ce-ac9b-53c90511ca39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive/CSE527 Computer Vision/Garg_Utkarsh_112672834_hw5'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/CSE527 Computer Vision/Garg_Utkarsh_112672834_hw5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94af3821-01e8-451f-9d8d-6b540f06ad4f",
        "id": "10-pNt1Rg_ue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# \\*write your codes for feature extraction (You can use multiple cells, this is just a place holder)\n",
        "# !wget 'http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-16 00:30:07--  http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar\n",
            "Resolving vision.cs.stonybrook.edu (vision.cs.stonybrook.edu)... 130.245.4.232\n",
            "Connecting to vision.cs.stonybrook.edu (vision.cs.stonybrook.edu)|130.245.4.232|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8658247680 (8.1G) [application/x-tar]\n",
            "Saving to: ‘UCF101_images.tar’\n",
            "\n",
            "UCF101_images.tar   100%[===================>]   8.06G  34.3MB/s    in 3m 9s   \n",
            "\n",
            "2019-11-16 00:33:16 (43.6 MB/s) - ‘UCF101_images.tar’ saved [8658247680/8658247680]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EGyhs7kkhBkL",
        "colab": {}
      },
      "source": [
        "# !tar -xkf './UCF101_images.tar' 2>/dev/null\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC19-w5DqioK",
        "colab_type": "code",
        "outputId": "69ca96cd-932d-4cf6-d107-87941f65ae8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H268BNJqpr1",
        "colab_type": "code",
        "outputId": "1e41d153-1a76-4a10-b33c-d1eb736225d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "vgg=models.vgg16(pretrained=True)\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "new_classifier = nn.Sequential(*list(vgg.classifier.children())[:1])\n",
        "vgg.classifier = new_classifier\n",
        "print(vgg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 80.1MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv4wcgVUqpjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "prep = T.Compose([ T.ToTensor(), normalize ])\n",
        "\n",
        "five_crop_transform = T.Compose([\n",
        "   T.FiveCrop((224,224)),\n",
        "   T.Lambda(lambda crops: torch.stack([prep(crop) for crop in crops])) \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kyM89CTqpnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Extraction of 25 class images: \n",
        "\n",
        "path='images/'\n",
        "video_folders=os.listdir(path)\n",
        "video_folders.sort()\n",
        "subset_folders = video_folders[:3625]\n",
        "output_folder = 'feature_data/'\n",
        "\n",
        "\n",
        "for each_vid in subset_folders:     #Processing first 25 classes,i.e ~110*25 videos => 3625 images.\n",
        "   frames = os.listdir(path+each_vid)\n",
        "   frames.sort()\n",
        "   vgg_all = []\n",
        "   for img_name in frames:\n",
        "       img = Image.open(path+each_vid+'/'+img_name)\n",
        "       img_crops = five_crop_transform(img)\n",
        "       ncrops, c, h, w = img_crops.size()\n",
        "       five_cropped_img_features = []\n",
        "       for cropped_img in img_crops:\n",
        "           feat_i = vgg(cropped_img.unsqueeze(0))\n",
        "           five_cropped_img_features.append(feat_i)\n",
        "       five = torch.stack(five_cropped_img_features)\n",
        "       mean_of_five_crops = five.mean(0).data\n",
        "       vgg_all.append(mean_of_five_crops)\n",
        "   vgg_for_vid = torch.stack(vgg_all)\n",
        "   d = {}\n",
        "   fe=vgg_for_vid.numpy()\n",
        "   fe=fe.reshape((25, 4096))\n",
        "   d['Feature'] = fe\n",
        "  #  print(each_vid, fe.shape , '===========================')\n",
        "   io.savemat(output_folder+each_vid.split('/')[-1]+'.mat', d)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ledqS2eUNeb4",
        "colab_type": "code",
        "outputId": "b1422f57-ca20-4206-b023-dda7b21daf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Using Map file to separate train and test data:\n",
        "\n",
        "mapping = pd.read_csv('annos/videos_labels_subsets.txt',sep=\"\\t\", header=None)\n",
        "mapping.columns = [\"video_name\", \"class_label\", \"set\"]\n",
        "print(mapping.head())\n",
        "data = mapping[mapping.class_label <= 25]\n",
        "train = data[data.set==1]\n",
        "test = data[data.set==2]\n",
        "print(mapping.shape, data.shape, train.shape, test.shape)\n",
        "xtrain=[]\n",
        "ytrain=[]\n",
        "xtest=[]\n",
        "ytest=[]\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    vid_name = row['video_name']\n",
        "    # print(vid_name)\n",
        "    mat_data = loadmat('feature_data/'+vid_name+'.mat')['Feature']\n",
        "    xtrain.append(mat_data)\n",
        "    ytrain.append(row['class_label'])\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    vid_name = row['video_name']\n",
        "    mat_data = loadmat('feature_data/'+vid_name+'.mat')['Feature']\n",
        "    xtest.append(mat_data)\n",
        "    ytest.append(row['class_label'])\n",
        "print(len(xtrain), len(ytrain), len(xtest), len(ytest))\n",
        "\n",
        "pickle.dump(np.stack(xtrain, axis=0), open('xtrain.p','wb'))\n",
        "pickle.dump(np.stack(ytrain, axis=0), open('ytrain.p','wb'))\n",
        "pickle.dump(np.stack(xtest, axis=0), open('xtest.p','wb'))\n",
        "pickle.dump(np.stack(ytest, axis=0), open('ytest.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  video_name  class_label  set\n",
            "0   v_000001            1    2\n",
            "1   v_000002            1    2\n",
            "2   v_000003            1    2\n",
            "3   v_000004            1    2\n",
            "4   v_000005            1    2\n",
            "(13320, 3) (3360, 3) (2409, 3) (951, 3)\n",
            "2409 2409 951 951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1yM4t5hR6cI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Loading data from Pickle file.\n",
        "\n",
        "xtrain = pickle.load(open('xtrain.p', 'rb'))\n",
        "xtest = pickle.load(open('xtest.p','rb'))\n",
        "ytrain = pickle.load(open('ytrain.p', 'rb'))\n",
        "ytest = pickle.load(open('ytest.p','rb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVn1tPZg9zxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shuffle train and test\n",
        "bundle = list(zip(xtrain, ytrain))\n",
        "random.shuffle(bundle)\n",
        "trD, trLb = zip(*bundle)\n",
        "\n",
        "bundleTest = list(zip(xtest, ytest))\n",
        "random.shuffle(bundleTest)\n",
        "tstD, tstLb = zip(*bundleTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UN74WLWpl7zQ"
      },
      "source": [
        "***\n",
        "***\n",
        "## **Problem 2.** Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVf0YjgI3AyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(data,labels,batch_size=4):\n",
        "  data_batches = []\n",
        "  label_batches = []\n",
        "  for i in (range(int(len(data)/batch_size))) :\n",
        "    minibatch_d = np.zeros((0,25,4096))\n",
        "    for each in data[i*batch_size: (i+1)*batch_size]:\n",
        "      each=np.reshape(each,(25,4096))\n",
        "      each=each[np.newaxis,:,:]\n",
        "      minibatch_d = np.vstack((minibatch_d,each))\n",
        "    \n",
        "    data_batches.append(torch.from_numpy(minibatch_d))\n",
        "    \n",
        "    minibatch_l=np.array(labels[i*batch_size: (i+1)*batch_size],dtype=int)\n",
        "    minibatch_l-=1\n",
        "    label_batches.append(torch.LongTensor(minibatch_l))\n",
        "\n",
        "  print(len(data_batches))\n",
        "  print(data_batches[0].shape)\n",
        "\n",
        "  print(len(label_batches))\n",
        "  return data_batches,label_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSDN775298es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trD_batches,trLb_batches=get_batches(trD,trLb)\n",
        "tstD_batches,tstLb_batches=get_batches(tstD,tstLb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChDE5me55O36",
        "colab_type": "code",
        "outputId": "f1bb824b-6824-4503-d7ea-a19de18b9e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install torchnet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchnet\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/b2/d7f70a85d3f6b0365517782632f150e3bbc2fb8e998cd69e27deba599aae/torchnet-0.0.4.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchnet) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchnet) (1.12.0)\n",
            "Collecting visdom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchnet) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (1.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (2.21.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (4.5.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (17.0.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/86/7e/035d19a73306278673039f0805b863be8798057cc1b4008b9c8c7d1d32a3/jsonpatch-1.24-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (4.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (3.0.4)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->visdom->torchnet) (0.46)\n",
            "Building wheels for collected packages: torchnet, visdom, torchfile\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-cp36-none-any.whl size=29743 sha256=13018c3d2a1e354e5bbee04e02cd9f58ec1e2bfd796e879d6a6d0f32e76918ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/03/fb/1c212c2f20905cdf97fe39022946cf16b8e66ed754a6663400\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655252 sha256=fc4714b66f6a00847f108c0d7c5761cc1232364741f56cc65220e5c8c8185ada\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=2b695aa421cb3e2f6cc87ff8288bbe7e9d018295f6b8322c0a213ffe9354b186\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built torchnet visdom torchfile\n",
            "Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom, torchnet\n",
            "Successfully installed jsonpatch-1.24 jsonpointer-2.0 torchfile-0.1.0 torchnet-0.0.4 visdom-0.1.8.9 websocket-client-0.56.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zd6-Wk1z4jp",
        "colab_type": "text"
      },
      "source": [
        "### Defining Helper Functions: (Accuracy / Classification Performance / Run_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TlABmQW4-Od3",
        "colab": {}
      },
      "source": [
        "def accuracy(model, dataset, labels):\n",
        "    \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() \n",
        "    for i in range(len(dataset)):\n",
        "        x_var = Variable(dataset[i].float())\n",
        "        y_var = Variable(labels[i])\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.max(1)\n",
        "        \n",
        "        num_correct += (preds.cpu().numpy() == y_var.cpu().numpy()).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE6nNUWiZn7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification_perf(model,dataloader):\n",
        "    print('\\nClass wise Accuracy Performance:\\n')\n",
        "    class_correct = list(0. for i in range(25))\n",
        "    class_total = list(0. for i in range(25))\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataloader)):\n",
        "            images = Variable(dataloader[i][0].cuda())\n",
        "            labels = Variable(dataloader[i][1].cuda().long())\n",
        "            outputs=model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(4):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "\n",
        "    for i in range(25):\n",
        "        print('Accuracy of %5s : %2d %%' % (\n",
        "            class_names[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvODWRunpn1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchnet as tnt\n",
        "num_class=25\n",
        "def run_epoch(dataset, labels , model, criterion, epoch, tstD,tstLb, optimizer=None):\n",
        "    \n",
        "    confusion_matrix = tnt.meter.ConfusionMeter(num_class)\n",
        "    acc = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    meter_loss = tnt.meter.AverageValueMeter()\n",
        "    model.train()\n",
        "    for i in range(len(dataset)):\n",
        "        sequence = dataset[i]\n",
        "        label = labels[i]\n",
        "        input_sequence_var = Variable(sequence.float())\n",
        "        input_label_var = Variable(label)\n",
        "\n",
        "        # compute output\n",
        "        output_logits = model(input_sequence_var)\n",
        "        loss = criterion(output_logits, input_label_var)\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        meter_loss.add(loss.data)\n",
        "        acc.add(output_logits.data, input_label_var.data)\n",
        "        confusion_matrix.add(output_logits.data, input_label_var.data)\n",
        "\n",
        "\n",
        "    print(' Epoch: %d  , Loss: %.4f,  Accuracy: %.2f'%(epoch, meter_loss.value()[0], acc.value()[0]))\n",
        "    print(\"Test set accuracy is\")\n",
        "    print(accuracy(model,tstD,tstLb))\n",
        "    return acc.value()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVVHrTR0CRR",
        "colab_type": "text"
      },
      "source": [
        "###Our 1st LSTM Model: Simple LSTM with 1 hidden and classification layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9IJnfL6xq-o",
        "colab_type": "code",
        "outputId": "6fcca312-84cf-4359-dbf1-ec34e4874c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model1, self).__init__()\n",
        "        \n",
        "        self.recurrent_layer1 = nn.LSTM(4096,500,num_layers=1,dropout=0.4,batch_first=True)\n",
        "        self.classify_layer1 = nn.Linear(500,25)\n",
        "        \n",
        "    \n",
        "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
        "        \n",
        "        rnn_outputs, (hn, cn) = self.recurrent_layer1(input)\n",
        "        logits = self.classify_layer1(rnn_outputs[:,-1])\n",
        "        \n",
        "        return logits\n",
        "        \n",
        "model1 = Model1()\n",
        "print(model1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model1(\n",
            "  (recurrent_layer1): LSTM(4096, 500, batch_first=True, dropout=0.4)\n",
            "  (classify_layer1): Linear(in_features=500, out_features=25, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eanuUPpg5I86",
        "colab_type": "code",
        "outputId": "617f2cb4-e42d-4b79-8d70-e25984907bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "optimizer = torch.optim.Adam(model1.parameters(),lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "for e in range(num_epochs):\n",
        "    run_epoch(trD_batches, trLb_batches, model1, criterion, e,tstD_batches,tstLb_batches,optimizer)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch: 0  , Loss: 1.8165,  Accuracy: 51.58\n",
            "Test set accuracy is\n",
            "Got 567 / 948 correct (59.81)\n",
            "None\n",
            " Epoch: 1  , Loss: 0.8908,  Accuracy: 77.20\n",
            "Test set accuracy is\n",
            "Got 648 / 948 correct (68.35)\n",
            "None\n",
            " Epoch: 2  , Loss: 0.5929,  Accuracy: 84.72\n",
            "Test set accuracy is\n",
            "Got 666 / 948 correct (70.25)\n",
            "None\n",
            " Epoch: 3  , Loss: 0.4745,  Accuracy: 88.25\n",
            "Test set accuracy is\n",
            "Got 680 / 948 correct (71.73)\n",
            "None\n",
            " Epoch: 4  , Loss: 0.3545,  Accuracy: 91.61\n",
            "Test set accuracy is\n",
            "Got 703 / 948 correct (74.16)\n",
            "None\n",
            " Epoch: 5  , Loss: 0.3182,  Accuracy: 92.32\n",
            "Test set accuracy is\n",
            "Got 717 / 948 correct (75.63)\n",
            "None\n",
            " Epoch: 6  , Loss: 0.2796,  Accuracy: 93.27\n",
            "Test set accuracy is\n",
            "Got 707 / 948 correct (74.58)\n",
            "None\n",
            " Epoch: 7  , Loss: 0.2428,  Accuracy: 94.02\n",
            "Test set accuracy is\n",
            "Got 709 / 948 correct (74.79)\n",
            "None\n",
            " Epoch: 8  , Loss: 0.2254,  Accuracy: 95.06\n",
            "Test set accuracy is\n",
            "Got 697 / 948 correct (73.52)\n",
            "None\n",
            " Epoch: 9  , Loss: 0.2455,  Accuracy: 93.73\n",
            "Test set accuracy is\n",
            "Got 726 / 948 correct (76.58)\n",
            "None\n",
            " Epoch: 10  , Loss: 0.1832,  Accuracy: 95.85\n",
            "Test set accuracy is\n",
            "Got 724 / 948 correct (76.37)\n",
            "None\n",
            " Epoch: 11  , Loss: 0.1718,  Accuracy: 96.14\n",
            "Test set accuracy is\n",
            "Got 740 / 948 correct (78.06)\n",
            "None\n",
            " Epoch: 12  , Loss: 0.1575,  Accuracy: 96.01\n",
            "Test set accuracy is\n",
            "Got 719 / 948 correct (75.84)\n",
            "None\n",
            " Epoch: 13  , Loss: 0.1579,  Accuracy: 96.22\n",
            "Test set accuracy is\n",
            "Got 751 / 948 correct (79.22)\n",
            "None\n",
            " Epoch: 14  , Loss: 0.1360,  Accuracy: 96.84\n",
            "Test set accuracy is\n",
            "Got 718 / 948 correct (75.74)\n",
            "None\n",
            " Epoch: 15  , Loss: 0.1224,  Accuracy: 97.34\n",
            "Test set accuracy is\n",
            "Got 718 / 948 correct (75.74)\n",
            "None\n",
            " Epoch: 16  , Loss: 0.1025,  Accuracy: 98.01\n",
            "Test set accuracy is\n",
            "Got 746 / 948 correct (78.69)\n",
            "None\n",
            " Epoch: 17  , Loss: 0.0945,  Accuracy: 97.92\n",
            "Test set accuracy is\n",
            "Got 711 / 948 correct (75.00)\n",
            "None\n",
            " Epoch: 18  , Loss: 0.1013,  Accuracy: 97.97\n",
            "Test set accuracy is\n",
            "Got 743 / 948 correct (78.38)\n",
            "None\n",
            " Epoch: 19  , Loss: 0.1097,  Accuracy: 97.51\n",
            "Test set accuracy is\n",
            "Got 726 / 948 correct (76.58)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeZ1AysN0KlQ",
        "colab_type": "text"
      },
      "source": [
        "###Model 2: LSTM with added dropout after recurrent_layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MbUD8P2xz7Z",
        "colab_type": "code",
        "outputId": "a9621272-d367-4c0e-96d5-2790a536f143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2, self).__init__()\n",
        "        \n",
        "        self.recurrent_layer1 = nn.LSTM(4096,500,num_layers=1,dropout=0.3,batch_first=True)\n",
        "        self.d=nn.Dropout(0.4)\n",
        "        self.classify_layer1 = nn.Linear(500,25)\n",
        "        \n",
        "    \n",
        "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
        "        \n",
        "        rnn_outputs, (hn, cn) = self.recurrent_layer1(input)\n",
        "        rnn_outputs=self.d(rnn_outputs)\n",
        "        logits = self.classify_layer1(rnn_outputs[:,-1])\n",
        "        \n",
        "        return logits\n",
        "\n",
        "model2 = Model2()\n",
        "print(model2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model2(\n",
            "  (recurrent_layer1): LSTM(4096, 500, batch_first=True, dropout=0.3)\n",
            "  (d): Dropout(p=0.4, inplace=False)\n",
            "  (classify_layer1): Linear(in_features=500, out_features=25, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDVjuBPLMMGd",
        "colab_type": "code",
        "outputId": "3fcc74a3-3a56-483d-fa7d-3b52465016ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "optimizer = torch.optim.Adam(model2.parameters(),lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "for e in range(num_epochs):\n",
        "    run_epoch(trD_batches, trLb_batches, model2, criterion, e,tstD_batches,tstLb_batches,optimizer)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch: 0  , Loss: 2.3688,  Accuracy: 30.44\n",
            "Test set accuracy is\n",
            "Got 431 / 948 correct (45.46)\n",
            "None\n",
            " Epoch: 1  , Loss: 1.5582,  Accuracy: 52.99\n",
            "Test set accuracy is\n",
            "Got 550 / 948 correct (58.02)\n",
            "None\n",
            " Epoch: 2  , Loss: 1.2047,  Accuracy: 64.04\n",
            "Test set accuracy is\n",
            "Got 578 / 948 correct (60.97)\n",
            "None\n",
            " Epoch: 3  , Loss: 0.9876,  Accuracy: 69.56\n",
            "Test set accuracy is\n",
            "Got 661 / 948 correct (69.73)\n",
            "None\n",
            " Epoch: 4  , Loss: 0.8870,  Accuracy: 72.13\n",
            "Test set accuracy is\n",
            "Got 652 / 948 correct (68.78)\n",
            "None\n",
            " Epoch: 5  , Loss: 0.7920,  Accuracy: 74.00\n",
            "Test set accuracy is\n",
            "Got 692 / 948 correct (73.00)\n",
            "None\n",
            " Epoch: 6  , Loss: 0.7441,  Accuracy: 75.62\n",
            "Test set accuracy is\n",
            "Got 668 / 948 correct (70.46)\n",
            "None\n",
            " Epoch: 7  , Loss: 0.7395,  Accuracy: 76.00\n",
            "Test set accuracy is\n",
            "Got 705 / 948 correct (74.37)\n",
            "None\n",
            " Epoch: 8  , Loss: 0.6321,  Accuracy: 79.36\n",
            "Test set accuracy is\n",
            "Got 699 / 948 correct (73.73)\n",
            "None\n",
            " Epoch: 9  , Loss: 0.6104,  Accuracy: 79.57\n",
            "Test set accuracy is\n",
            "Got 675 / 948 correct (71.20)\n",
            "None\n",
            " Epoch: 10  , Loss: 0.6044,  Accuracy: 80.11\n",
            "Test set accuracy is\n",
            "Got 720 / 948 correct (75.95)\n",
            "None\n",
            " Epoch: 11  , Loss: 0.5718,  Accuracy: 81.77\n",
            "Test set accuracy is\n",
            "Got 693 / 948 correct (73.10)\n",
            "None\n",
            " Epoch: 12  , Loss: 0.5871,  Accuracy: 80.44\n",
            "Test set accuracy is\n",
            "Got 694 / 948 correct (73.21)\n",
            "None\n",
            " Epoch: 13  , Loss: 0.5798,  Accuracy: 80.02\n",
            "Test set accuracy is\n",
            "Got 736 / 948 correct (77.64)\n",
            "None\n",
            " Epoch: 14  , Loss: 0.4766,  Accuracy: 84.18\n",
            "Test set accuracy is\n",
            "Got 731 / 948 correct (77.11)\n",
            "None\n",
            " Epoch: 15  , Loss: 0.4773,  Accuracy: 83.55\n",
            "Test set accuracy is\n",
            "Got 697 / 948 correct (73.52)\n",
            "None\n",
            " Epoch: 16  , Loss: 0.5054,  Accuracy: 81.94\n",
            "Test set accuracy is\n",
            "Got 731 / 948 correct (77.11)\n",
            "None\n",
            " Epoch: 17  , Loss: 0.4431,  Accuracy: 84.97\n",
            "Test set accuracy is\n",
            "Got 722 / 948 correct (76.16)\n",
            "None\n",
            " Epoch: 18  , Loss: 0.4308,  Accuracy: 85.42\n",
            "Test set accuracy is\n",
            "Got 739 / 948 correct (77.95)\n",
            "None\n",
            " Epoch: 19  , Loss: 0.4046,  Accuracy: 85.80\n",
            "Test set accuracy is\n",
            "Got 739 / 948 correct (77.95)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3SLv9hn0NRI",
        "colab_type": "text"
      },
      "source": [
        "###Model 3: LSTM with ReLu activation on recurrent layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5NA0_YDfueq",
        "colab_type": "code",
        "outputId": "560157da-c3ab-486c-ba32-2bf785061ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model3, self).__init__()\n",
        "        \n",
        "        self.recurrent_layer1 = nn.LSTM(4096,500,num_layers=1,dropout=0.4,batch_first=True)\n",
        "        self.act=nn.ReLU(inplace=True)\n",
        "        self.classify_layer1 = nn.Linear(500,25)\n",
        "        \n",
        "    \n",
        "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
        "        \n",
        "        rnn_outputs, (hn, cn) = self.recurrent_layer1(input)\n",
        "        rnn_outputs=self.act(rnn_outputs)\n",
        "        logits = self.classify_layer1(rnn_outputs[:,-1])\n",
        "        \n",
        "        return logits\n",
        "\n",
        "model3 = Model3()\n",
        "print(model3)\n",
        "optimizer = torch.optim.Adam(model3.parameters(),lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "for e in range(num_epochs):\n",
        "    run_epoch(trD_batches, trLb_batches, model3, criterion, e,tstD_batches,tstLb_batches,optimizer)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model3(\n",
            "  (recurrent_layer1): LSTM(4096, 500, batch_first=True, dropout=0.4)\n",
            "  (act): ReLU(inplace=True)\n",
            "  (classify_layer1): Linear(in_features=500, out_features=25, bias=True)\n",
            ")\n",
            " Epoch: 0  , Loss: 1.7229,  Accuracy: 69.19\n",
            "Test set accuracy is\n",
            "Got 757 / 948 correct (79.85)\n",
            "None\n",
            " Epoch: 1  , Loss: 0.6340,  Accuracy: 93.40\n",
            "Test set accuracy is\n",
            "Got 776 / 948 correct (81.86)\n",
            "None\n",
            " Epoch: 2  , Loss: 0.2923,  Accuracy: 97.88\n",
            "Test set accuracy is\n",
            "Got 787 / 948 correct (83.02)\n",
            "None\n",
            " Epoch: 3  , Loss: 0.1482,  Accuracy: 99.29\n",
            "Test set accuracy is\n",
            "Got 795 / 948 correct (83.86)\n",
            "None\n",
            " Epoch: 4  , Loss: 0.0807,  Accuracy: 99.83\n",
            "Test set accuracy is\n",
            "Got 802 / 948 correct (84.60)\n",
            "None\n",
            " Epoch: 5  , Loss: 0.0466,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 802 / 948 correct (84.60)\n",
            "None\n",
            " Epoch: 6  , Loss: 0.0276,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 803 / 948 correct (84.70)\n",
            "None\n",
            " Epoch: 7  , Loss: 0.0172,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 809 / 948 correct (85.34)\n",
            "None\n",
            " Epoch: 8  , Loss: 0.0113,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 808 / 948 correct (85.23)\n",
            "None\n",
            " Epoch: 9  , Loss: 0.0078,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 814 / 948 correct (85.86)\n",
            "None\n",
            " Epoch: 10  , Loss: 0.0053,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 809 / 948 correct (85.34)\n",
            "None\n",
            " Epoch: 11  , Loss: 0.0037,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 810 / 948 correct (85.44)\n",
            "None\n",
            " Epoch: 12  , Loss: 0.0026,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 813 / 948 correct (85.76)\n",
            "None\n",
            " Epoch: 13  , Loss: 0.0018,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 814 / 948 correct (85.86)\n",
            "None\n",
            " Epoch: 14  , Loss: 0.0013,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 813 / 948 correct (85.76)\n",
            "None\n",
            " Epoch: 15  , Loss: 0.0009,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 814 / 948 correct (85.86)\n",
            "None\n",
            " Epoch: 16  , Loss: 0.0007,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 813 / 948 correct (85.76)\n",
            "None\n",
            " Epoch: 17  , Loss: 0.0005,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 810 / 948 correct (85.44)\n",
            "None\n",
            " Epoch: 18  , Loss: 0.0004,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 808 / 948 correct (85.23)\n",
            "None\n",
            " Epoch: 19  , Loss: 0.0003,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 810 / 948 correct (85.44)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTugvkdjyJ7K",
        "colab_type": "text"
      },
      "source": [
        "##Model 4: LSTM with BatchNorm,ReLu Activation and dropout on classification layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al39dd2EyJI5",
        "colab_type": "code",
        "outputId": "d55127f5-9f35-47a0-85eb-b27ea9073281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Model4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model4, self).__init__()\n",
        "        self.recurrent_layer = torch.nn.LSTM(4096,100, num_layers = 1, dropout=0.5, batch_first=True)\n",
        "        self.classify_layer = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(25,4,3),\n",
        "            torch.nn.BatchNorm1d(4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.5),\n",
        "            Flatten(),\n",
        "            torch.nn.Linear(392,25))\n",
        "    \n",
        "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
        "        rnn_outputs, (hn, cn) = self.recurrent_layer(input)\n",
        "        logits = self.classify_layer(rnn_outputs)\n",
        "        return logits\n",
        "      \n",
        "model4 = Model4()\n",
        "print(model4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model4(\n",
            "  (recurrent_layer): LSTM(4096, 100, batch_first=True, dropout=0.5)\n",
            "  (classify_layer): Sequential(\n",
            "    (0): Conv1d(25, 4, kernel_size=(3,), stride=(1,))\n",
            "    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Flatten()\n",
            "    (5): Linear(in_features=392, out_features=25, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rxN5uA-aIsE",
        "colab_type": "code",
        "outputId": "5d442f4f-f22c-4783-a0de-8a1752b1ce76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "optimizer = torch.optim.Adam(model4.parameters(),lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "for e in range(num_epochs):\n",
        "    run_epoch(trD_batches, trLb_batches, model4, criterion, e,tstD_batches,tstLb_batches,optimizer)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model3(\n",
            "  (recurrent_layer): LSTM(4096, 100, batch_first=True, dropout=0.5)\n",
            "  (classify_layer): Sequential(\n",
            "    (0): Conv1d(25, 4, kernel_size=(3,), stride=(1,))\n",
            "    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Flatten()\n",
            "    (5): Linear(in_features=392, out_features=25, bias=True)\n",
            "  )\n",
            ")\n",
            " Epoch: 0  , Loss: 2.0573,  Accuracy: 37.00\n",
            "Test set accuracy is\n",
            "Got 520 / 948 correct (54.85)\n",
            "None\n",
            " Epoch: 1  , Loss: 0.9883,  Accuracy: 67.52\n",
            "Test set accuracy is\n",
            "Got 683 / 948 correct (72.05)\n",
            "None\n",
            " Epoch: 2  , Loss: 0.6441,  Accuracy: 78.65\n",
            "Test set accuracy is\n",
            "Got 689 / 948 correct (72.68)\n",
            "None\n",
            " Epoch: 3  , Loss: 0.4468,  Accuracy: 84.55\n",
            "Test set accuracy is\n",
            "Got 690 / 948 correct (72.78)\n",
            "None\n",
            " Epoch: 4  , Loss: 0.4211,  Accuracy: 85.96\n",
            "Test set accuracy is\n",
            "Got 714 / 948 correct (75.32)\n",
            "None\n",
            " Epoch: 5  , Loss: 0.3475,  Accuracy: 88.79\n",
            "Test set accuracy is\n",
            "Got 689 / 948 correct (72.68)\n",
            "None\n",
            " Epoch: 6  , Loss: 0.3495,  Accuracy: 87.71\n",
            "Test set accuracy is\n",
            "Got 695 / 948 correct (73.31)\n",
            "None\n",
            " Epoch: 7  , Loss: 0.2845,  Accuracy: 91.53\n",
            "Test set accuracy is\n",
            "Got 728 / 948 correct (76.79)\n",
            "None\n",
            " Epoch: 8  , Loss: 0.1995,  Accuracy: 93.44\n",
            "Test set accuracy is\n",
            "Got 717 / 948 correct (75.63)\n",
            "None\n",
            " Epoch: 9  , Loss: 0.1955,  Accuracy: 93.69\n",
            "Test set accuracy is\n",
            "Got 711 / 948 correct (75.00)\n",
            "None\n",
            " Epoch: 10  , Loss: 0.1816,  Accuracy: 93.56\n",
            "Test set accuracy is\n",
            "Got 749 / 948 correct (79.01)\n",
            "None\n",
            " Epoch: 11  , Loss: 0.1865,  Accuracy: 93.36\n",
            "Test set accuracy is\n",
            "Got 703 / 948 correct (74.16)\n",
            "None\n",
            " Epoch: 12  , Loss: 0.2115,  Accuracy: 93.15\n",
            "Test set accuracy is\n",
            "Got 747 / 948 correct (78.80)\n",
            "None\n",
            " Epoch: 13  , Loss: 0.2368,  Accuracy: 92.11\n",
            "Test set accuracy is\n",
            "Got 715 / 948 correct (75.42)\n",
            "None\n",
            " Epoch: 14  , Loss: 0.1626,  Accuracy: 94.60\n",
            "Test set accuracy is\n",
            "Got 742 / 948 correct (78.27)\n",
            "None\n",
            " Epoch: 15  , Loss: 0.1206,  Accuracy: 96.10\n",
            "Test set accuracy is\n",
            "Got 733 / 948 correct (77.32)\n",
            "None\n",
            " Epoch: 16  , Loss: 0.1086,  Accuracy: 97.01\n",
            "Test set accuracy is\n",
            "Got 753 / 948 correct (79.43)\n",
            "None\n",
            " Epoch: 17  , Loss: 0.1226,  Accuracy: 95.97\n",
            "Test set accuracy is\n",
            "Got 735 / 948 correct (77.53)\n",
            "None\n",
            " Epoch: 18  , Loss: 0.1295,  Accuracy: 95.97\n",
            "Test set accuracy is\n",
            "Got 758 / 948 correct (79.96)\n",
            "None\n",
            " Epoch: 19  , Loss: 0.1167,  Accuracy: 95.72\n",
            "Test set accuracy is\n",
            "Got 758 / 948 correct (79.96)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beZ22uuXVDgF",
        "colab_type": "text"
      },
      "source": [
        "**RESULT: Model 3 (LSTM) trained on 25 class images performed the best with test set accuracy of 85.86%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v0znm2TMmsDZ"
      },
      "source": [
        "---\n",
        "---\n",
        "## **Problem 3.** Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq2ABURMN1aV",
        "colab_type": "text"
      },
      "source": [
        "###SVM Classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF3JC_APT9Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Flattening each 4096*25 sized image \n",
        "def get_flat_data(dataset,r=4096,c=25):\n",
        "  temp=np.zeros((0,r*c))\n",
        "  for each in dataset:\n",
        "    temp=np.vstack((temp,np.reshape(each,(1,r*c))))\n",
        "  return temp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXttWpBjUChB",
        "colab_type": "code",
        "outputId": "a565c8fd-7910-4d6d-b6a9-f099db5bf96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "training_data=get_flat_data(trD)\n",
        "testing_data=get_flat_data(tstD)\n",
        "  \n",
        "print(training_data.shape)\n",
        "print(testing_data.shape)\n",
        "print(trLb)\n",
        "print(tstLb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2409, 102400)\n",
            "(951, 102400)\n",
            "('11', '23', '15', '22', '3', '22', '1', '15', '4', '15', '18', '16', '9', '9', '8', '24', '16', '16', '12', '12', '25', '1', '13', '5', '9', '4', '20', '24', '12', '13', '10', '1', '5', '18', '23', '22', '16', '9', '23', '11', '22', '10', '17', '3', '6', '23', '10', '19', '3', '21', '5', '20', '21', '25', '1', '17', '16', '8', '23', '21', '6', '2', '13', '13', '22', '12', '9', '3', '22', '7', '6', '9', '20', '22', '3', '16', '2', '6', '2', '9', '1', '2', '4', '1', '6', '3', '20', '14', '5', '9', '10', '18', '3', '3', '3', '1', '16', '15', '16', '1', '17', '9', '24', '7', '3', '18', '15', '24', '4', '16', '8', '24', '9', '17', '18', '23', '15', '8', '5', '1', '21', '4', '9', '11', '6', '22', '23', '23', '12', '9', '15', '17', '5', '11', '14', '13', '23', '12', '19', '3', '16', '11', '1', '18', '13', '12', '19', '18', '11', '8', '2', '9', '24', '24', '11', '14', '1', '20', '19', '4', '2', '4', '20', '9', '6', '8', '2', '6', '11', '20', '17', '10', '24', '17', '24', '16', '8', '5', '6', '13', '25', '16', '15', '20', '20', '6', '24', '6', '14', '9', '24', '12', '18', '8', '25', '5', '8', '25', '19', '1', '6', '16', '21', '2', '2', '6', '14', '25', '7', '3', '10', '12', '24', '23', '12', '4', '20', '15', '17', '15', '15', '24', '18', '15', '3', '24', '7', '9', '10', '18', '18', '12', '6', '5', '1', '5', '4', '8', '11', '16', '13', '10', '25', '8', '1', '13', '3', '24', '11', '15', '24', '10', '18', '13', '6', '15', '7', '15', '14', '19', '8', '10', '11', '21', '7', '2', '18', '18', '16', '19', '1', '5', '3', '9', '4', '18', '17', '3', '21', '23', '2', '6', '6', '8', '15', '5', '4', '6', '18', '18', '24', '14', '18', '18', '13', '24', '17', '4', '16', '13', '12', '15', '9', '25', '21', '2', '17', '18', '7', '17', '22', '9', '16', '14', '10', '23', '16', '2', '18', '15', '6', '3', '6', '23', '12', '16', '23', '11', '23', '13', '6', '7', '11', '24', '8', '23', '5', '12', '20', '24', '24', '17', '24', '23', '5', '16', '12', '13', '11', '5', '10', '6', '10', '10', '7', '24', '15', '16', '25', '7', '25', '2', '20', '12', '16', '5', '20', '22', '23', '2', '11', '8', '7', '11', '18', '5', '15', '23', '22', '12', '11', '11', '1', '15', '5', '16', '22', '14', '2', '11', '23', '2', '6', '19', '18', '20', '17', '15', '23', '8', '3', '20', '3', '9', '9', '17', '17', '14', '10', '20', '11', '17', '1', '12', '2', '10', '8', '8', '20', '2', '25', '22', '20', '18', '20', '10', '18', '8', '22', '16', '9', '24', '11', '25', '23', '25', '18', '14', '18', '23', '11', '3', '11', '20', '1', '22', '21', '8', '20', '23', '6', '18', '1', '22', '5', '25', '16', '20', '15', '15', '23', '4', '21', '5', '17', '18', '8', '6', '24', '6', '18', '4', '19', '17', '21', '24', '17', '17', '6', '15', '22', '7', '4', '5', '18', '17', '4', '17', '23', '24', '4', '10', '7', '24', '17', '7', '23', '9', '1', '22', '7', '23', '14', '18', '6', '14', '14', '6', '8', '20', '1', '12', '1', '10', '18', '4', '7', '7', '25', '3', '15', '7', '4', '24', '22', '11', '9', '16', '11', '1', '23', '6', '18', '3', '25', '17', '11', '15', '2', '8', '20', '6', '20', '16', '16', '17', '17', '22', '6', '23', '7', '12', '21', '25', '25', '23', '4', '5', '22', '1', '22', '3', '12', '2', '11', '2', '21', '4', '17', '18', '23', '24', '24', '7', '22', '20', '13', '16', '22', '10', '17', '2', '20', '7', '4', '17', '23', '17', '2', '5', '17', '9', '10', '16', '24', '21', '4', '20', '1', '14', '7', '7', '6', '22', '19', '21', '9', '10', '5', '6', '13', '25', '4', '23', '23', '16', '4', '1', '10', '25', '16', '24', '15', '12', '22', '24', '3', '8', '10', '3', '13', '4', '10', '17', '6', '21', '17', '1', '15', '14', '9', '10', '1', '17', '17', '24', '13', '19', '9', '4', '12', '3', '11', '6', '25', '18', '6', '16', '5', '20', '3', '10', '17', '16', '25', '10', '15', '13', '24', '6', '13', '23', '14', '18', '12', '22', '16', '15', '25', '12', '17', '18', '9', '10', '3', '12', '17', '25', '16', '20', '9', '11', '8', '4', '12', '20', '22', '8', '3', '22', '1', '3', '11', '17', '10', '10', '13', '22', '7', '19', '8', '6', '25', '23', '7', '6', '23', '16', '9', '5', '11', '10', '23', '24', '1', '15', '23', '3', '19', '9', '14', '22', '7', '1', '3', '6', '4', '12', '13', '3', '25', '21', '24', '17', '2', '25', '3', '2', '5', '17', '1', '1', '4', '17', '17', '24', '24', '4', '12', '7', '13', '5', '3', '7', '20', '17', '17', '14', '4', '18', '22', '25', '15', '9', '12', '8', '11', '10', '22', '16', '17', '4', '21', '23', '11', '12', '17', '16', '7', '3', '8', '2', '20', '12', '3', '15', '12', '7', '1', '25', '14', '11', '18', '14', '14', '25', '11', '24', '9', '5', '16', '25', '25', '6', '17', '21', '16', '6', '3', '2', '9', '18', '2', '13', '19', '20', '22', '11', '15', '14', '19', '12', '9', '15', '5', '14', '8', '14', '15', '20', '12', '9', '12', '17', '16', '22', '1', '7', '10', '23', '10', '16', '25', '1', '18', '19', '1', '24', '4', '24', '14', '22', '22', '17', '23', '11', '19', '24', '13', '17', '1', '5', '10', '14', '12', '4', '25', '1', '4', '18', '16', '11', '18', '21', '6', '3', '9', '14', '16', '10', '17', '5', '23', '7', '5', '11', '1', '3', '4', '16', '16', '10', '5', '18', '11', '22', '2', '10', '25', '3', '8', '17', '2', '2', '6', '15', '22', '14', '14', '3', '15', '3', '21', '12', '7', '12', '16', '23', '10', '10', '11', '6', '17', '13', '12', '1', '20', '3', '16', '19', '24', '22', '18', '11', '17', '4', '6', '10', '21', '15', '12', '23', '17', '25', '7', '13', '20', '13', '13', '21', '23', '17', '5', '3', '2', '6', '15', '19', '3', '6', '9', '10', '20', '21', '13', '14', '25', '2', '18', '19', '11', '21', '2', '19', '1', '9', '18', '8', '20', '22', '16', '1', '20', '14', '12', '22', '21', '21', '22', '24', '7', '13', '17', '12', '16', '22', '16', '10', '22', '5', '25', '20', '8', '16', '5', '20', '4', '10', '7', '7', '7', '9', '18', '9', '24', '10', '8', '13', '1', '2', '8', '4', '10', '11', '17', '20', '17', '13', '17', '18', '12', '9', '22', '6', '25', '22', '6', '6', '12', '3', '24', '21', '13', '17', '19', '24', '4', '24', '11', '7', '13', '13', '19', '1', '10', '13', '13', '11', '4', '23', '15', '22', '16', '12', '23', '24', '21', '21', '10', '21', '2', '11', '19', '18', '16', '10', '1', '21', '22', '11', '4', '25', '8', '2', '22', '16', '20', '15', '9', '20', '18', '21', '12', '8', '9', '8', '24', '21', '19', '7', '23', '12', '4', '16', '7', '21', '9', '10', '13', '17', '6', '18', '10', '13', '2', '6', '4', '22', '8', '20', '25', '5', '10', '24', '24', '5', '7', '1', '10', '2', '2', '7', '1', '12', '7', '21', '12', '7', '21', '3', '12', '17', '5', '5', '8', '11', '23', '19', '9', '9', '2', '17', '14', '24', '8', '6', '12', '2', '16', '18', '13', '15', '20', '3', '3', '21', '6', '21', '20', '1', '12', '21', '9', '12', '23', '10', '24', '4', '17', '2', '10', '17', '24', '13', '9', '22', '16', '24', '22', '19', '24', '1', '19', '23', '23', '12', '16', '17', '7', '17', '13', '9', '2', '20', '9', '5', '7', '24', '22', '22', '11', '19', '2', '13', '1', '7', '25', '18', '14', '16', '13', '5', '18', '23', '12', '1', '8', '6', '11', '24', '23', '5', '12', '15', '18', '3', '5', '7', '20', '3', '5', '8', '22', '1', '6', '18', '1', '22', '2', '5', '19', '4', '23', '20', '18', '24', '25', '6', '21', '6', '14', '3', '6', '11', '25', '13', '23', '16', '5', '15', '8', '8', '5', '4', '10', '15', '1', '19', '7', '3', '18', '1', '21', '17', '25', '8', '24', '18', '20', '24', '14', '6', '15', '13', '10', '1', '10', '19', '4', '16', '21', '5', '9', '25', '15', '17', '17', '8', '23', '11', '25', '1', '23', '22', '10', '6', '3', '6', '11', '21', '20', '15', '7', '17', '8', '14', '24', '16', '6', '21', '3', '25', '19', '8', '11', '13', '7', '22', '12', '12', '17', '3', '2', '18', '6', '8', '16', '11', '9', '7', '19', '22', '17', '6', '22', '19', '13', '10', '25', '13', '3', '13', '19', '5', '24', '24', '22', '1', '10', '15', '24', '21', '13', '17', '22', '21', '22', '6', '12', '7', '13', '19', '12', '22', '21', '13', '2', '21', '7', '7', '17', '16', '3', '23', '7', '20', '7', '8', '18', '2', '3', '11', '1', '17', '14', '22', '22', '8', '1', '17', '4', '4', '3', '3', '17', '9', '10', '23', '14', '4', '14', '21', '9', '6', '4', '24', '20', '11', '18', '7', '7', '9', '12', '19', '15', '3', '5', '11', '20', '20', '12', '17', '22', '9', '8', '12', '10', '19', '10', '21', '13', '10', '24', '24', '24', '25', '18', '24', '12', '14', '19', '2', '13', '25', '8', '21', '11', '13', '9', '3', '14', '10', '25', '2', '20', '18', '21', '25', '23', '3', '17', '19', '15', '11', '10', '18', '20', '10', '13', '13', '12', '17', '12', '8', '3', '21', '20', '18', '4', '18', '11', '19', '25', '19', '9', '7', '3', '3', '10', '19', '19', '2', '2', '8', '7', '2', '20', '1', '12', '3', '15', '12', '2', '7', '8', '8', '24', '23', '16', '12', '7', '23', '3', '3', '3', '18', '13', '18', '25', '8', '23', '19', '8', '18', '10', '6', '23', '24', '8', '21', '24', '23', '12', '5', '3', '2', '3', '14', '4', '5', '14', '17', '18', '12', '19', '16', '3', '7', '13', '6', '10', '14', '1', '23', '1', '17', '20', '24', '18', '13', '23', '14', '11', '10', '6', '17', '16', '13', '8', '7', '24', '21', '19', '6', '15', '8', '11', '3', '8', '18', '13', '4', '7', '21', '19', '23', '14', '6', '13', '9', '18', '24', '12', '25', '1', '3', '22', '16', '24', '10', '24', '16', '3', '11', '4', '20', '14', '14', '7', '21', '13', '4', '4', '23', '12', '9', '5', '16', '24', '11', '7', '8', '15', '12', '11', '10', '16', '21', '1', '15', '3', '1', '18', '1', '3', '8', '23', '7', '6', '12', '6', '12', '4', '8', '16', '9', '17', '21', '23', '22', '16', '17', '25', '6', '6', '10', '16', '7', '13', '15', '11', '20', '15', '20', '12', '7', '6', '14', '13', '7', '15', '9', '6', '16', '25', '14', '10', '1', '2', '3', '4', '18', '24', '11', '7', '11', '10', '11', '7', '16', '7', '16', '21', '22', '4', '12', '12', '18', '2', '4', '1', '1', '3', '21', '23', '5', '8', '15', '20', '8', '21', '8', '20', '16', '16', '21', '18', '7', '6', '6', '4', '16', '2', '9', '10', '13', '13', '23', '14', '24', '17', '9', '4', '12', '12', '10', '21', '16', '8', '22', '1', '16', '10', '7', '11', '25', '23', '22', '3', '15', '24', '8', '8', '25', '13', '1', '15', '17', '19', '11', '22', '9', '25', '23', '3', '4', '11', '3', '7', '12', '4', '7', '18', '9', '2', '7', '4', '6', '6', '6', '10', '4', '5', '21', '8', '23', '1', '17', '25', '20', '14', '17', '2', '19', '10', '18', '8', '24', '16', '4', '25', '6', '22', '5', '10', '1', '24', '12', '24', '7', '17', '13', '15', '6', '1', '5', '18', '7', '6', '5', '24', '20', '1', '25', '14', '8', '3', '17', '20', '13', '25', '17', '4', '10', '18', '9', '9', '15', '16', '7', '12', '21', '16', '13', '15', '4', '7', '16', '20', '20', '4', '11', '25', '1', '9', '6', '23', '24', '22', '9', '12', '23', '9', '3', '23', '7', '19', '23', '6', '19', '8', '11', '23', '4', '24', '10', '13', '24', '2', '9', '18', '22', '7', '20', '2', '1', '12', '20', '3', '24', '8', '14', '15', '7', '17', '9', '24', '16', '8', '18', '12', '3', '18', '2', '10', '10', '15', '23', '17', '10', '6', '10', '11', '14', '24', '17', '6', '14', '10', '5', '3', '10', '17', '24', '24', '17', '8', '22', '9', '17', '20', '4', '13', '6', '24', '2', '5', '4', '3', '8', '11', '18', '13', '4', '17', '10', '15', '25', '7', '15', '6', '1', '19', '3', '9', '4', '11', '23', '15', '20', '24', '11', '12', '25', '23', '17', '24', '4', '7', '1', '1', '21', '6', '22', '10', '25', '10', '4', '9', '11', '11', '16', '13', '18', '22', '1', '2', '8', '17', '22', '14', '5', '11', '10', '24', '10', '3', '8', '23', '7', '9', '23', '11', '17', '12', '24', '14', '1', '23', '6', '13', '13', '11', '12', '9', '4', '3', '6', '16', '16', '8', '12', '16', '22', '5', '7', '23', '5', '2', '22', '16', '6', '1', '1', '9', '19', '22', '20', '8', '17', '1', '20', '1', '17', '1', '6', '20', '14', '10', '14', '13', '22', '1', '25', '20', '1', '14', '10', '1', '20', '7', '14', '18', '5', '12', '12', '13', '16', '15', '16', '9', '24', '20', '1', '2', '5', '8', '6', '5', '11', '16', '16', '12', '3', '2', '3', '14', '13', '9', '6', '6', '6', '12', '23', '7', '18', '23', '5', '20', '19', '18', '21', '8', '8', '22', '16', '16', '7', '2', '19', '10', '1', '21', '2', '20', '13', '21', '6', '16', '14', '10', '8', '10', '13', '3', '7', '20', '25', '24', '4', '9', '14', '2', '3', '7', '19', '16', '2', '8', '15', '12', '15', '4', '23', '7', '5', '8', '6', '16', '24', '12', '12', '10', '21', '19', '11', '4', '24', '4', '24', '14', '23', '19', '9', '11', '8', '20', '11', '22', '9', '11', '4', '14', '19', '20', '20', '13', '7', '3', '7', '15', '19', '25', '7', '23', '9', '24', '18', '10', '14', '12', '9', '12', '21', '19', '11', '13', '11', '14', '1', '22', '11', '24', '6', '19', '13', '22', '1', '17', '15', '12', '6', '12', '6', '7', '22', '3', '10', '16', '21', '20', '23', '18', '21', '12', '19', '2', '12', '4', '9', '22', '6', '5', '15', '22', '16', '22', '5', '19', '12', '10', '4', '2', '4', '12', '24', '21', '7', '14', '25', '6', '10', '24', '19', '17', '4', '20', '21', '17', '24', '9', '16', '19', '10', '2', '25', '16', '22', '24', '8', '16', '23', '15', '4', '5', '23', '17', '9', '10', '8', '14', '8', '4', '19', '7', '9', '22', '8', '16', '9', '9', '2', '18', '10', '8', '7', '10', '20', '16', '22', '5', '6', '14', '12', '8', '13', '21', '20', '2', '10', '5', '5', '13', '1', '19', '15', '25', '2', '2', '17', '23', '20', '13', '15', '24', '5', '15', '1', '13', '11', '19', '25', '11', '7', '19', '22', '24', '6', '22', '23', '16', '18', '22', '14', '4', '25', '12', '4', '13', '8', '4', '1', '20', '3', '19', '14', '2', '4', '18', '3', '20', '17', '15', '16', '1', '11', '24', '23', '4', '14', '21', '23', '24', '24', '1', '7', '3', '12', '3')\n",
            "('23', '4', '15', '18', '17', '9', '11', '6', '10', '3', '24', '14', '3', '18', '13', '3', '6', '7', '25', '24', '9', '12', '2', '7', '1', '3', '25', '1', '18', '24', '13', '3', '22', '24', '3', '10', '20', '1', '10', '10', '8', '23', '20', '25', '8', '22', '15', '3', '7', '6', '9', '23', '20', '14', '17', '16', '8', '18', '10', '16', '13', '7', '17', '23', '9', '10', '5', '16', '13', '16', '1', '3', '6', '10', '4', '11', '18', '1', '13', '1', '16', '1', '3', '24', '17', '17', '23', '22', '24', '3', '12', '22', '8', '10', '16', '17', '1', '10', '15', '20', '1', '13', '12', '20', '24', '11', '23', '6', '11', '17', '24', '19', '24', '12', '20', '1', '8', '3', '11', '16', '18', '6', '24', '13', '19', '25', '18', '13', '5', '9', '3', '6', '1', '19', '21', '15', '18', '11', '5', '13', '9', '21', '12', '6', '14', '6', '3', '13', '1', '24', '24', '19', '14', '7', '24', '11', '3', '14', '15', '5', '5', '2', '22', '18', '23', '18', '23', '17', '11', '21', '19', '9', '16', '22', '6', '17', '10', '16', '5', '16', '11', '21', '11', '2', '17', '15', '10', '4', '11', '16', '12', '23', '25', '16', '8', '16', '4', '22', '24', '4', '7', '21', '6', '8', '24', '8', '1', '12', '25', '13', '6', '2', '3', '2', '8', '9', '18', '15', '5', '6', '25', '7', '2', '15', '18', '7', '1', '11', '7', '17', '14', '25', '9', '9', '21', '19', '17', '18', '12', '11', '7', '24', '6', '11', '3', '17', '8', '21', '17', '9', '24', '9', '19', '8', '3', '9', '12', '14', '14', '14', '24', '5', '21', '24', '14', '10', '25', '4', '13', '5', '7', '18', '20', '8', '18', '1', '24', '1', '1', '19', '18', '22', '8', '17', '23', '8', '14', '11', '22', '22', '10', '6', '3', '20', '16', '13', '23', '8', '4', '14', '25', '21', '4', '25', '24', '23', '1', '16', '7', '12', '6', '22', '3', '1', '5', '10', '14', '11', '21', '12', '9', '9', '4', '19', '16', '1', '17', '2', '7', '8', '13', '4', '18', '18', '2', '12', '7', '10', '3', '23', '23', '20', '5', '1', '11', '2', '13', '16', '6', '20', '10', '23', '14', '17', '4', '25', '23', '17', '3', '6', '13', '5', '20', '22', '1', '13', '8', '7', '20', '17', '22', '21', '12', '25', '18', '3', '11', '10', '16', '14', '6', '2', '17', '20', '17', '10', '1', '3', '22', '10', '15', '4', '11', '12', '24', '8', '17', '24', '1', '15', '10', '10', '18', '15', '6', '11', '12', '24', '19', '7', '9', '3', '6', '10', '23', '13', '4', '3', '4', '14', '21', '7', '1', '17', '22', '1', '1', '25', '7', '21', '22', '17', '23', '25', '24', '5', '16', '17', '16', '12', '21', '12', '8', '22', '1', '12', '7', '19', '2', '4', '6', '17', '13', '5', '17', '16', '4', '23', '14', '17', '1', '11', '12', '20', '7', '18', '23', '3', '7', '9', '22', '25', '20', '19', '9', '10', '14', '14', '12', '22', '23', '23', '17', '14', '2', '11', '15', '17', '8', '13', '22', '18', '4', '24', '15', '13', '9', '6', '22', '18', '4', '7', '16', '16', '25', '15', '6', '10', '20', '25', '5', '21', '12', '9', '22', '20', '12', '13', '24', '23', '24', '1', '4', '22', '23', '7', '16', '20', '20', '24', '24', '11', '10', '2', '5', '8', '3', '2', '8', '24', '21', '20', '13', '6', '10', '24', '21', '11', '2', '23', '2', '10', '6', '14', '3', '8', '19', '4', '19', '8', '16', '21', '17', '25', '13', '9', '13', '5', '21', '14', '25', '13', '15', '18', '14', '8', '23', '20', '8', '20', '2', '12', '22', '2', '5', '17', '23', '7', '2', '17', '12', '21', '25', '10', '22', '19', '25', '2', '17', '16', '10', '12', '9', '10', '24', '1', '19', '11', '18', '2', '10', '21', '23', '22', '13', '24', '11', '5', '1', '25', '7', '6', '15', '2', '17', '4', '3', '19', '19', '16', '7', '24', '25', '11', '23', '3', '1', '24', '20', '1', '6', '14', '6', '19', '6', '1', '12', '15', '10', '8', '8', '10', '22', '17', '24', '8', '3', '14', '7', '10', '18', '6', '7', '18', '9', '7', '20', '24', '8', '5', '17', '25', '22', '20', '15', '20', '16', '13', '21', '16', '23', '2', '18', '22', '4', '22', '21', '19', '18', '4', '22', '8', '4', '2', '9', '24', '5', '5', '7', '15', '21', '23', '15', '20', '1', '5', '10', '6', '2', '7', '4', '13', '25', '7', '3', '14', '13', '19', '6', '13', '17', '12', '9', '14', '20', '21', '9', '24', '4', '24', '11', '15', '16', '21', '8', '23', '25', '16', '11', '3', '7', '16', '7', '8', '7', '19', '22', '22', '1', '4', '11', '18', '15', '22', '15', '25', '19', '16', '3', '3', '3', '11', '11', '16', '16', '13', '11', '10', '20', '9', '24', '5', '1', '21', '6', '11', '4', '21', '11', '9', '3', '2', '23', '9', '18', '10', '18', '6', '12', '3', '1', '4', '25', '15', '10', '2', '18', '5', '7', '10', '9', '17', '19', '23', '17', '5', '24', '1', '1', '10', '9', '16', '3', '3', '24', '2', '20', '17', '18', '6', '14', '20', '25', '15', '12', '21', '2', '4', '21', '25', '15', '10', '15', '24', '10', '12', '16', '20', '25', '12', '18', '11', '15', '9', '8', '20', '13', '18', '6', '17', '5', '13', '12', '16', '17', '9', '14', '11', '7', '24', '16', '6', '5', '6', '23', '6', '22', '1', '23', '16', '6', '5', '9', '4', '12', '22', '4', '21', '21', '18', '17', '9', '6', '13', '19', '17', '17', '7', '10', '20', '10', '25', '16', '2', '10', '10', '22', '9', '12', '22', '7', '10', '7', '13', '4', '6', '12', '24', '1', '24', '7', '16', '2', '12', '17', '24', '14', '5', '19', '12', '22', '3', '8', '14', '4', '16', '12', '14', '17', '21', '5', '19', '13', '2', '20', '7', '17', '4', '19', '13', '15', '9', '15', '11', '14', '20', '12', '12', '10', '7', '1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG3W_T7ScTQ6",
        "colab_type": "text"
      },
      "source": [
        "**SVM TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI-Ap_tF44_T",
        "colab_type": "code",
        "outputId": "a418465d-8e27-4935-c0c3-6921e594d826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.svm import SVC  \n",
        "svm = SVC(kernel='linear', C = 1.0)  \n",
        "svm.fit(training_data, np.array(trLb,dtype=int))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCRK2pascWQc",
        "colab_type": "text"
      },
      "source": [
        "**Making predictions using Trained SVM and Evaluation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqhMRuRP8KjQ",
        "colab_type": "code",
        "outputId": "94994d30-9545-4c8d-92d3-7edb150f8a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_label=svm.predict(testing_data)\n",
        "from sklearn.metrics import accuracy_score  \n",
        "acc=accuracy_score(np.array(tstLb,dtype=int), pred_label)\n",
        "print(acc*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82.75499474237644 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UZ0jEGaUo_r",
        "colab_type": "text"
      },
      "source": [
        "**SVM Accuracy on test data set of 25 classes : 82.75499474237644 %**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbzpDeRRNuHj",
        "colab_type": "text"
      },
      "source": [
        "## **Bonus**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TejvCd9BWyp1",
        "colab_type": "text"
      },
      "source": [
        "###Separating train and test data for entire UCF101 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrIMObdTGwCO",
        "colab_type": "code",
        "outputId": "f7a5e52f-2f49-4817-a140-f93279402b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "################################\n",
        "# using mapping file to separate train and test data\n",
        "# using pickle to save separated train and test data\n",
        "###############################\n",
        "mapping = pd.read_csv('annos/videos_labels_subsets.txt',sep=\"\\t\", header=None)\n",
        "mapping.columns = [\"video_name\", \"class_label\", \"set\"]\n",
        "print(mapping.head())\n",
        "data = mapping[mapping.class_label <= 101]\n",
        "train = data[data.set==1]\n",
        "test = data[data.set==2]\n",
        "print(mapping.shape, data.shape, train.shape, test.shape)\n",
        "xtrain=[]\n",
        "ytrain=[]\n",
        "xtest=[]\n",
        "ytest=[]\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    vid_name = row['video_name']\n",
        "    # print(vid_name)\n",
        "    mat_data = loadmat('bonus_features/'+vid_name+'.mat')['Feature']\n",
        "    xtrain.append(mat_data)\n",
        "    ytrain.append(row['class_label'])\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    vid_name = row['video_name']\n",
        "    mat_data = loadmat('bonus_features/'+vid_name+'.mat')['Feature']\n",
        "    xtest.append(mat_data)\n",
        "    ytest.append(row['class_label'])\n",
        "print(len(xtrain), len(ytrain), len(xtest), len(ytest))\n",
        "\n",
        "pickle.dump(np.stack(xtrain, axis=0), open('xtrain_bonus.p','wb'))\n",
        "pickle.dump(np.stack(ytrain, axis=0), open('ytrain_bonus.p','wb'))\n",
        "pickle.dump(np.stack(xtest, axis=0), open('xtest_bonus.p','wb'))\n",
        "pickle.dump(np.stack(ytest, axis=0), open('ytest_bonus.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  video_name  class_label  set\n",
            "0   v_000001            1    2\n",
            "1   v_000002            1    2\n",
            "2   v_000003            1    2\n",
            "3   v_000004            1    2\n",
            "4   v_000005            1    2\n",
            "(13320, 3) (13320, 3) (9537, 3) (3783, 3)\n",
            "9537 9537 3783 3783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LV52bbOCjGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Loading train and test data/labels using Pickle.\n",
        "xtrain = pickle.load(open('xtrain_bonus.p', 'rb'))\n",
        "xtest = pickle.load(open('xtest_bonus.p','rb'))\n",
        "ytrain = pickle.load(open('ytrain_bonus.p', 'rb'))\n",
        "ytest = pickle.load(open('ytest_bonus.p','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4vM37iqoTs",
        "colab_type": "code",
        "outputId": "4d04f370-af52-4783-8575-b22a91ecae63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Checking size of UCF101 dataset:\n",
        "import os\n",
        "vid=(os.listdir(\"bonus_features/\"))\n",
        "print(len(vid))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtVXiwW_oa8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shuffle train and test\n",
        "bundle_train = list(zip(xtrain, ytrain))\n",
        "random.shuffle(bundle_train)\n",
        "trData, trLb = zip(*bundle_train)\n",
        "\n",
        "bundle_test = list(zip(xtest, ytest))\n",
        "random.shuffle(bundle_test)\n",
        "testData, testLb = zip(*bundle_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U2LuCZmo8M-",
        "colab_type": "code",
        "outputId": "7ace1a44-7205-49fa-8482-27421fe269b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "trainD_batches,trainLb_batches=get_batches(trData, trLb)\n",
        "testD_batches,testLb_batches=get_batches(testData, testLb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1192\n",
            "torch.Size([8, 25, 4096])\n",
            "1192\n",
            "472\n",
            "torch.Size([8, 25, 4096])\n",
            "472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzIvGnIEqNfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchnet as tnt\n",
        "num_class=101\n",
        "def run_epoch2(dataset, labels , model, criterion, epoch, tstD,tstLb, optimizer=None):\n",
        "    \n",
        "    confusion_matrix = tnt.meter.ConfusionMeter(num_class)\n",
        "    acc = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    meter_loss = tnt.meter.AverageValueMeter()\n",
        "    model.train()\n",
        "    for i in range(len(dataset)):\n",
        "        sequence = dataset[i]\n",
        "        label = labels[i]\n",
        "        input_sequence_var = Variable(sequence.float())\n",
        "        input_label_var = Variable(label)\n",
        "\n",
        "        # compute output\n",
        "        output_logits = model(input_sequence_var)\n",
        "        loss = criterion(output_logits, input_label_var)\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        meter_loss.add(loss.data)\n",
        "        acc.add(output_logits.data, input_label_var.data)\n",
        "        confusion_matrix.add(output_logits.data, input_label_var.data)\n",
        "\n",
        "\n",
        "    print(' Epoch: %d  , Loss: %.4f,  Accuracy: %.2f'%(epoch, meter_loss.value()[0], acc.value()[0]))\n",
        "    print(\"Test set accuracy is\")\n",
        "    print(accuracy(model,tstD,tstLb))\n",
        "    return acc.value()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU3zC4cdo8KL",
        "colab_type": "code",
        "outputId": "2968d604-43dd-49ee-f062-4d3bde8fc494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class BestModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BestModel, self).__init__()\n",
        "        \n",
        "        self.recurrent_layer1 = nn.LSTM(4096,500,num_layers=1,dropout=0.4,batch_first=True)\n",
        "        self.act=nn.ReLU(inplace=True)\n",
        "        self.classify_layer1 = nn.Linear(500,101)\n",
        "        \n",
        "    \n",
        "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
        "        \n",
        "        rnn_outputs, (hn, cn) = self.recurrent_layer1(input)\n",
        "        rnn_outputs=self.act(rnn_outputs)\n",
        "        logits = self.classify_layer1(rnn_outputs[:,-1])\n",
        "        \n",
        "        return logits\n",
        "\n",
        "model = BestModel()\n",
        "print(model)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "for e in range(num_epochs):\n",
        "    run_epoch2(trainD_batches, trainLb_batches, model, criterion, e,testD_batches,testLb_batches,optimizer)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BestModel(\n",
            "  (recurrent_layer1): LSTM(4096, 500, batch_first=True, dropout=0.4)\n",
            "  (act): ReLU(inplace=True)\n",
            "  (classify_layer1): Linear(in_features=500, out_features=101, bias=True)\n",
            ")\n",
            " Epoch: 0  , Loss: 3.0723,  Accuracy: 44.97\n",
            "Test set accuracy is\n",
            "Got 2164 / 3780 correct (57.25)\n",
            "None\n",
            " Epoch: 1  , Loss: 1.5666,  Accuracy: 76.15\n",
            "Test set accuracy is\n",
            "Got 2568 / 3780 correct (67.94)\n",
            "None\n",
            " Epoch: 2  , Loss: 0.9286,  Accuracy: 87.21\n",
            "Test set accuracy is\n",
            "Got 2686 / 3780 correct (71.06)\n",
            "None\n",
            " Epoch: 3  , Loss: 0.5776,  Accuracy: 93.20\n",
            "Test set accuracy is\n",
            "Got 2759 / 3780 correct (72.99)\n",
            "None\n",
            " Epoch: 4  , Loss: 0.3666,  Accuracy: 96.61\n",
            "Test set accuracy is\n",
            "Got 2785 / 3780 correct (73.68)\n",
            "None\n",
            " Epoch: 5  , Loss: 0.2335,  Accuracy: 98.31\n",
            "Test set accuracy is\n",
            "Got 2779 / 3780 correct (73.52)\n",
            "None\n",
            " Epoch: 6  , Loss: 0.1500,  Accuracy: 99.37\n",
            "Test set accuracy is\n",
            "Got 2801 / 3780 correct (74.10)\n",
            "None\n",
            " Epoch: 7  , Loss: 0.0913,  Accuracy: 99.80\n",
            "Test set accuracy is\n",
            "Got 2806 / 3780 correct (74.23)\n",
            "None\n",
            " Epoch: 8  , Loss: 0.0553,  Accuracy: 99.96\n",
            "Test set accuracy is\n",
            "Got 2828 / 3780 correct (74.81)\n",
            "None\n",
            " Epoch: 9  , Loss: 0.0342,  Accuracy: 99.98\n",
            "Test set accuracy is\n",
            "Got 2837 / 3780 correct (75.05)\n",
            "None\n",
            " Epoch: 10  , Loss: 0.0179,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2846 / 3780 correct (75.29)\n",
            "None\n",
            " Epoch: 11  , Loss: 0.0103,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2843 / 3780 correct (75.21)\n",
            "None\n",
            " Epoch: 12  , Loss: 0.0064,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2852 / 3780 correct (75.45)\n",
            "None\n",
            " Epoch: 13  , Loss: 0.0030,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2894 / 3780 correct (76.56)\n",
            "None\n",
            " Epoch: 14  , Loss: 0.0055,  Accuracy: 99.95\n",
            "Test set accuracy is\n",
            "Got 2411 / 3780 correct (63.78)\n",
            "None\n",
            " Epoch: 15  , Loss: 0.0325,  Accuracy: 99.75\n",
            "Test set accuracy is\n",
            "Got 2852 / 3780 correct (75.45)\n",
            "None\n",
            " Epoch: 16  , Loss: 0.0029,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2898 / 3780 correct (76.67)\n",
            "None\n",
            " Epoch: 17  , Loss: 0.0014,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2893 / 3780 correct (76.53)\n",
            "None\n",
            " Epoch: 18  , Loss: 0.0007,  Accuracy: 100.00\n",
            "Test set accuracy is\n",
            "Got 2895 / 3780 correct (76.59)\n",
            "None\n",
            " Epoch: 19  , Loss: 0.0169,  Accuracy: 99.77\n",
            "Test set accuracy is\n",
            "Got 2750 / 3780 correct (72.75)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8neMuC4ViHl",
        "colab_type": "text"
      },
      "source": [
        " **RESULT FOR FULL UCF101 DATASET:** LSTM Model accuracy on test dataset for 101 class images is 76.67%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cL4Y3nHBkwmb"
      },
      "source": [
        "## **Problem 4.** Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ybKpm4Rbxws",
        "colab_type": "text"
      },
      "source": [
        "**Report can be found in the uploaded .zip file to Blackboard.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vb4Wlzw2jYcJ"
      },
      "source": [
        "## Submission\n",
        "---\n",
        "**Runnable source code in ipynb file and a pdf report are required**.\n",
        "\n",
        "The report should be of 3 to 4 pages describing what you have done and learned in this homework and report performance of your model. If you have tried multiple methods, please compare your results. If you are using any external code, please cite it in your report. Note that this homework is designed to help you explore and get familiar with the techniques. The final grading will be largely based on your prediction accuracy and the different methods you tried (different architectures and parameters).\n",
        "\n",
        "Please indicate clearly in your report what model you have tried, what techniques you applied to improve the performance and report their accuracies. The report should be concise and include the highlights of your efforts.\n",
        "The naming convention for report is **Surname_Givenname_SBUID_report*.pdf**\n",
        "\n",
        "When submitting your .zip file through blackboard, please\n",
        "-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n",
        "\n",
        "This zip file should include:\n",
        "```\n",
        "Surname_Givenname_SBUID_hw*\n",
        "        |---Surname_Givenname_SBUID_hw*.ipynb\n",
        "        |---Surname_Givenname_SBUID_hw*.pdf\n",
        "        |---Surname_Givenname_SBUID_report*.pdf\n",
        "```\n",
        "\n",
        "For instance, student Michael Jordan should submit a zip file named \"Jordan_Michael_111134567_hw5.zip\" for homework5 in this structure:\n",
        "```\n",
        "Jordan_Michael_111134567_hw5\n",
        "        |---Jordan_Michael_111134567_hw5.ipynb\n",
        "        |---Jordan_Michael_111134567_hw5.pdf\n",
        "        |---Jordan_Michael_111134567_report*.pdf\n",
        "```\n",
        "\n",
        "The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link**. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. \n",
        "\n",
        "Then right click this folder, click ***Get shareable link***, in the People textfield, enter two TA's emails: ***bo.cao.1@stonybrook.edu*** and ***sayontan.ghosh@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n",
        "\n",
        "Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n",
        "\n",
        "You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n",
        "\n",
        "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrOtiYfmn7U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}